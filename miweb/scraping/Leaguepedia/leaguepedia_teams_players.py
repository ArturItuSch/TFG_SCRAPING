"""
`leaguepedia_teams_players.py`

Este script realiza scraping sobre la wiki colaborativa [Leaguepedia](https://lol.fandom.com),
extrayendo informaci贸n actualizada de los **equipos** y **jugadores activos** en la LEC (League of Legends
EMEA Championship) para una temporada concreta (por defecto: Spring 2025).

La informaci贸n recolectada incluye:
- Enlaces a p谩ginas de equipos.
- Datos b谩sicos de los jugadores: nombre, pa铆s, residencia, rol, fechas de contrato, IDs de soloqueue, imagen.
- Informaci贸n de equipos: nombre oficial, pa铆s, regi贸n, propietarios, patrocinadores, head coach y logo.

Tambi茅n descarga y organiza las **im谩genes de jugadores y equipos**, genera diccionarios estructurados
y guarda datos en archivos JSON listos para su inserci贸n en base de datos.

### Funciones destacadas:

- `get_team_links(url)`: Extrae los enlaces a todos los equipos participantes desde la p谩gina del split.
- `get_extra_player_data(url)`: Obtiene informaci贸n extendida de cada jugador desde su perfil individual.
- `get_player_data()`: Itera por los equipos y compone una lista completa de los jugadores actuales.
- `get_team_data()`: Extrae informaci贸n detallada de cada equipo (estructura organizativa, fundaci贸n, logo, etc).
- `download_image(ruta, url)`: Descarga una imagen desde una URL y la guarda en el disco.
- `convertir_fecha()`, `get_html()`: Funciones auxiliares para limpieza y conexi贸n.

Este script es ejecutado directamente o bien invocado desde el scheduler o un script orquestador
(`importar_datos.py`) para mantener actualizados los datos del split actual.

**Requiere conexi贸n a Internet y acceso a la plataforma Leaguepedia.**
"""
# Librer铆as est谩ndar y externas utilizadas
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os
import sys
import re
from datetime import datetime
import random
import time

# Definici贸n de directorios clave y rutas del proyecto
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..','..'))
sys.path.insert(0, PROJECT_ROOT)

# Importaci贸n de rutas desde archivo de configuraci贸n
from Resources.rutas import * # Rutas globales del proyecto definidas externamente en el m贸dulo Resources

# Directorios y rutas para guardar datos
TEAM_IMAGES_DIR = os.path.join(CARPETA_IMAGENES_TEAMS)
PLAYER_IMAGES_DIR = os.path.join(CARPETA_IMAGENES_PLAYERS)
PLAYER_INSTALATION_DATA = os.path.join(JSON_INSTALATION_PLAYERS, "players_data_leguepedia.json")
TEAMS_INSTALATION_JSON = os.path.join(JSON_INSTALATION_TEAMS, "teams_data_leguepedia.json")
LOGO_TEAMS_PATH = os.path.join(JSON_PATH_TEAMS_LOGOS, "teams_logos_path.json")


# Headers HTTP simulando navegador real para evitar bloqueos
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "es-ES,es;q=0.9,en;q=0.8",
    "Referer": "https://www.google.com/",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "DNT": "1", 
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "same-origin",
    "Sec-Fetch-User": "?1",
    "Cache-Control": "max-age=0",
}

def obtener_headers_dinamicos():
    """
    Genera encabezados HTTP din谩micos para simular distintos navegadores y evitar bloqueos por scraping.

    Cambia aleatoriamente el `User-Agent` y el `Referer` para cada petici贸n, eligiendo entre varias
    opciones populares.

    Returns:
        dict: Diccionario de encabezados HTTP con `User-Agent` y `Referer` aleatorios.
    """
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/112.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/112.0"
    ]
    
    referers = [
        "https://www.google.com/",
        "https://www.bing.com/",
        "https://www.yahoo.com/"
    ]
    
    headers["User-Agent"] = random.choice(user_agents)
    headers["Referer"] = random.choice(referers)
    
    return headers

def convertir_fecha(fecha_str):
    """
    Convierte una cadena de texto con formato de fecha en ingl茅s (por ejemplo, 'March 10, 2002')
    al formato ISO est谩ndar (`YYYY-MM-DD`).

    Tambi茅n elimina cualquier comentario entre par茅ntesis que acompa帽e la fecha (como nacionalidades o notas).

    Args:
        fecha_str (str): Fecha en formato ingl茅s, potencialmente con anotaciones entre par茅ntesis.

    Returns:
        str or None: Fecha convertida al formato 'YYYY-MM-DD', o `None` si ocurre un error en el formato.
    """
    try:
        fecha_limpia = re.sub(r'\(.*?\)', '', fecha_str).strip()
        fecha = datetime.strptime(fecha_limpia, "%B %d, %Y")
        return fecha.strftime("%Y-%m-%d")
    except ValueError:
        return None
    except Exception as e:
        print(f"Error al convertir la fecha: {e}")
        return None


def get_html(url, timeout=10):
    """
    Realiza una solicitud HTTP GET a la URL especificada y devuelve la respuesta si es exitosa.

    Se utilizan encabezados din谩micos para simular navegaci贸n humana, y se maneja una amplia
    gama de excepciones para garantizar robustez ante errores comunes de red.

    Args:
        url (str): Direcci贸n web a la que se realizar谩 la solicitud.
        timeout (int): Tiempo m谩ximo de espera en segundos antes de cancelar la solicitud (por defecto: 10).

    Returns:
        requests.Response or None: Objeto de respuesta si la solicitud fue exitosa, `None` si fall贸.
    """
    try:
        time.sleep(1)
        headers = obtener_headers_dinamicos()
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response
    except requests.exceptions.Timeout:
        print("锔 Tiempo de espera agotado.")
    except requests.exceptions.TooManyRedirects:
        print("锔 Demasiadas redirecciones.")
    except requests.exceptions.SSLError:
        print("锔 Error SSL.")
    except requests.exceptions.ConnectionError:
        print("锔 Error de conexi贸n.")
    except requests.exceptions.HTTPError as err:
        print(f"锔 Error HTTP: {err}")
    except requests.exceptions.RequestException as e:
        print(f"锔 Error desconocido: {e}")

def get_team_links(url):
    """
    Obtiene los enlaces a las p谩ginas de todos los equipos que participan en una temporada espec铆fica
    de la LEC desde Leaguepedia.

    Esta funci贸n analiza la p谩gina principal de la temporada (`url`) y busca enlaces HTML con la clase
    `catlink-teams`, t铆picamente utilizada en Leaguepedia para vincular a las p谩ginas de equipos.

    Args:
        url (str): URL de la p谩gina de la temporada en Leaguepedia (por ejemplo, Spring 2025).

    Returns:
        set: Conjunto de URLs absolutas a las p谩ginas de los equipos. Se devuelve un set vac铆o si ocurre
        alg煤n error durante la solicitud o el parseo.
    """
    base_url = 'https://lol.fandom.com/'
    try:
        response = get_html(url, headers)
        if response is None:
            print(f"No se pudo obtener respuesta de la URL: {url} en get_team_links")
            return set()

        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a', class_='catlink-teams')
        urls = set()
        for link in links:
            href = link.get('href')
            if href:
                full_url = urljoin(base_url, href)
                urls.add(full_url)

        return urls

    except requests.exceptions.RequestException as e:
        print(f"Error al realizar la solicitud a {url}: {e}")
        return set()
    except Exception as e:
        print(f"Error inesperado al procesar los enlaces del equipo: {e}")
        return set()

def get_extra_player_data(url):
    """
    Extrae informaci贸n adicional de un jugador desde su perfil individual en Leaguepedia.

    Esta funci贸n accede a la URL del perfil del jugador y obtiene detalles como:
    - Imagen del jugador (descargada y almacenada localmente).
    - Fecha de nacimiento (formateada como `YYYY-MM-DD`).
    - Identificadores de SoloQueue (por regi贸n).

    Se utiliza BeautifulSoup para parsear la tabla `infoboxPlayer` y descargar directamente la imagen
    desde la ruta proporcionada en el HTML.

    Args:
        url (str): URL absoluta del perfil del jugador en Leaguepedia.

    Returns:
        dict or None: Diccionario con los campos `image`, `birthday`, `soloqueue_ids` si se extraen correctamente,
        o `None` si ocurre un error en la carga o procesamiento de datos.
    """
    try:
        response = get_html(url, headers)
        if response is None:
            print(f"No se pudo obtener respuesta de la URL: {url} en get_player_data")
            return None
        player_data = {}
        soup = BeautifulSoup(response.text, 'html.parser')
        try:
            table = soup.find('table', id = 'infoboxPlayer')
            if not table:
                print(f"No se encontr贸 la tabla de informaci贸n del jugador en: {url}")
                return None
            player_data = {}
            rows = table.find_all('tr')
            for row in rows:
                td = row.find('td', class_='infobox-wide')
                if td is not None:
                    img_link_tag = table.find('a', class_='image')
                    if img_link_tag and img_link_tag.has_attr('href'):
                        player_image = download_image(os.path.join(PLAYER_IMAGES_DIR, f"{url.split('/')[-1]}.png"), img_link_tag['href'])
                        if player_image is not None:
                            player_data['image'] = player_image
                            continue
                        else:
                            print(f"Error al descargar la imagen del jugador: {url}")
                            player_data['image'] = None
                    else:
                        print(f"No se encontr贸 la imagen del jugador en: {url}")
                        return None                            
                # Conseguir el cumplea帽os del jugador y sus nicknames de SoloQ
                cells = row.find_all('td')
                if len(cells) == 2:
                    label = cells[0].get_text(strip=True)
                    if label == "Birthday":
                        birthday_text = cells[1].get_text(strip=True)
                        player_data['birthday'] = convertir_fecha(birthday_text)
                        continue
                    elif label == "Soloqueue IDs":
                        entries = []
                        bold_tags = cells[1].find_all('b')
                        for b in bold_tags:
                            region = b.get_text(strip=True).replace(':', '')
                            nickname = b.next_sibling.strip() if b.next_sibling else ''
                            entries.append(f"{region}: {nickname}")
                        player_data['soloqueue_ids'] = ', '.join(entries)
                        continue              
        except Exception as e:
            print(f"Error al procesar la tabla de informaci贸n del jugador: {e}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"Error al realizar la solicitud a {url}: {e}")
        return None
    except Exception as e:
        print(f"Error inesperado al procesar los datos del jugador: {e}")
        return None
    return player_data
                                 
            

def get_player_data():
    """
    Extrae informaci贸n detallada de todos los jugadores activos en la LEC desde Leaguepedia.

    Recorre las p谩ginas de los equipos participantes en el split actual (por defecto: Spring 2025)
    y obtiene para cada jugador:
    - Nombre de invocador y nombre real
    - Residencia y nacionalidad
    - Rol dentro del equipo
    - Fecha de entrada y finalizaci贸n de contrato
    - IDs de SoloQueue
    - Fecha de nacimiento
    - Imagen (descargada y almacenada localmente)

    Esta funci贸n hace uso de `get_team_links` para obtener los equipos, y `get_extra_player_data` para
    ampliar los datos de cada jugador desde su perfil individual.

    Returns:
        list[dict]: Lista de diccionarios, cada uno representando a un jugador con sus campos estructurados.
    """
    all_player_data = []
    urls = get_team_links("https://lol.fandom.com/wiki/LEC/2025_Season/Spring_Season")  
    for url in urls:
        try:
            response = get_html(url, headers)
            if response is None:
                print(f"No se pudo obtener respuesta de la URL: {url} en get_team_data")
                continue  # Saltar a la siguiente URL si no se obtiene respuesta

            soup = BeautifulSoup(response.text, 'html.parser')
            tabla = soup.find('table', class_='team-members')
            if not tabla:
                print(f"No se encontr贸 la tabla de miembros en: {url}")
                continue  # Saltar a la siguiente URL si no se encuentra la tabla

            tbody = tabla.find('tbody')
            if not tbody:
                print(f"No se encontr贸 el cuerpo de la tabla en: {url}")
                continue  # Saltar a la siguiente URL si no se encuentra el cuerpo de la tabla

            rows = tbody.find_all('tr')

            for row in rows:
                celdas = row.find_all('td')
                if len(celdas) < 7:
                    continue 
                try:
                    
                    residencia = celdas[0].get_text(strip=True)
                    pais = celdas[1].find('span')['title'] if celdas[1].find('span') else ''
                    jugador = celdas[2].get_text(strip=True)
                    nombre_real = celdas[3].get_text(strip=True)
                    rol = celdas[4].find('span', class_='markup-object-name').get_text(strip=True)
                    contrato = celdas[5].get_text(strip=True)
                    spans = celdas[6].find_all('span')
                    fecha_union = spans[1].get_text(strip=True) if len(spans) > 1 else ''
                
                    try:
                        enlace_tag = celdas[2].find('a')
                        url_relativa = enlace_tag['href'] if enlace_tag else ''
                        url_jugador = urljoin('https://lol.fandom.com', url_relativa)
                        datos_complementarios = get_extra_player_data(url_jugador)
                    except Exception as e:
                        print(f"Error al obtener datos complementarios del jugador: {e}")
                        datos_complementarios = {}
                        
                    miembro = {
                        'jugador': jugador,
                        'nombre': nombre_real,
                        'residencia': residencia,
                        'rol': rol,
                        'equipo': re.sub(r'\s*\(.*?\)\s*', '', url.split('/')[-1].replace('_', ' ').replace('Season', '').strip()),
                        'pais': pais,
                        'nacimiento': datos_complementarios.get('birthday', None),
                        'soloqueue_ids': datos_complementarios.get('soloqueue_ids', None),
                        'contratado_hasta': contrato,
                        'contratado_desde': fecha_union,
                        'imagen': datos_complementarios.get('image', None),
                    }
                    all_player_data.append(miembro)

                except Exception as e:
                    print(f"Error al procesar una fila: {e}")
                    continue  

        except requests.exceptions.RequestException as e:
            print(f"Error de red al acceder a {url}: {e}")
            continue 

        except Exception as e:
            print(f"Error inesperado al procesar los datos del equipo en {url}: {e}")
            continue  
    return all_player_data


def download_image(ruta_completa, url_imagen):
    """
    Descarga una imagen desde una URL y la guarda en la ruta especificada dentro del sistema de archivos local.

    Si la imagen ya existe en el disco, se omite la descarga para evitar redundancias. Al finalizar,
    se devuelve la ruta relativa normalizada desde el directorio `Resources/`, compatible con el uso
    en rutas web o en base de datos.

    Args:
        ruta_completa (str): Ruta absoluta donde se debe guardar la imagen.
        url_imagen (str): URL desde la cual descargar la imagen.

    Returns:
        str or None: Ruta relativa normalizada si la descarga o verificaci贸n es exitosa, `None` si ocurre un error.
    """
    try:
        if os.path.exists(ruta_completa):
            print(f"Imagen ya existe en {ruta_completa}, se omite la descarga.")
        else:
            print(f"Descargando imagen desde {url_imagen} a {ruta_completa}...")
            img_data = requests.get(url_imagen, timeout=10)
            img_data.raise_for_status()
            with open(ruta_completa, 'wb') as f:
                f.write(img_data.content)
            print(f"Imagen guardada correctamente en {ruta_completa}.")
        
        ruta_normalizada = ruta_completa.replace('\\', '/')
        index = ruta_normalizada.find('Resources/')
        if index != -1:
            ruta_relativa = ruta_normalizada[index + len('Resources/'):]
        else:
            ruta_relativa = os.path.relpath(ruta_completa).replace('\\', '/')
        
        return ruta_relativa

    except Exception as e:
        print(f"Error al descargar la imagen: {e}")
        return None
    
def get_team_data():
    """
    Extrae informaci贸n detallada de los equipos activos en la LEC para una temporada espec铆fica desde Leaguepedia.

    Recorre las p谩ginas de cada equipo listadas en la p谩gina del split actual (por defecto: Spring 2025),
    y recopila los siguientes datos estructurados:
    - Nombre oficial del equipo (limpiado y formateado)
    - Logo del equipo (descargado localmente y guardado como ruta relativa)
    - Pa铆s de origen
    - Regi贸n competitiva
    - Propietario del equipo
    - Entrenador principal (Head Coach)
    - Patrocinadores (Partners)
    - Fecha de fundaci贸n

    Returns:
        list[dict]: Lista de diccionarios con la informaci贸n clave de cada equipo participante.
    """
    informacion_equipos = []
    urls = get_team_links("https://lol.fandom.com/wiki/LEC/2025_Season/Spring_Season")
    if not urls:
        print("No se encontraron enlaces de equipos.")
        return []
    for url in urls:
        try:
            response = get_html(url, headers)
            if response is None:
                print(f"No se pudo obtener respuesta de la URL: {url}")
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')

            # Buscar la tabla de informaci贸n del equipo
            table = soup.select_one('table#infoboxTeam')
            if not table:
                print(f"No se encontr贸 la tabla de informaci贸n del equipo en: {url}")
                continue
            equipo = {
                'nombre_equipo': None,
                'logo': None,
                'pais': None,
                'region': None,
                'propietario': None,
                'head_coach': None,
                'partners': None,
                'fecha_fundacion': None
            }

            # Nombre del equipo
            nombre = table.select_one('th.infobox-title')
            if nombre:
                texto = nombre.get_text(strip=True)
                # Eliminar todo lo que est茅 entre par茅ntesis y los par茅ntesis
                texto_sin_parentesis = re.sub(r'\s*\(.*?\)\s*', '', texto)
                equipo['nombre_equipo'] = texto_sin_parentesis.replace(' ', '_').replace('Season', '').strip()

            # Logo
            logo_img = table.select_one('img[data-src]')
            if logo_img:
                logo_path = download_image(os.path.join(TEAM_IMAGES_DIR, f"{equipo['nombre_equipo']}.png"), logo_img['data-src'])
                if logo_path is not None:
                    equipo['logo'] = logo_path
            else:
                print(f"No se encontr贸 la imagen del logo en: {url}")

            # Pa铆s (Org Location)
            org_location_row = table.find('td', class_='infobox-label', string='Org Location')
            if org_location_row:
                pais_td = org_location_row.find_next_sibling('td')
                pais_span = pais_td.select_one('span.markup-object-name')
                equipo['pais'] = pais_span.get_text(strip=True) if pais_span else pais_td.get_text(strip=True)

            # Regi贸n
            region_icon = table.select_one('tr.infobox-region div.region-icon')
            if region_icon:
                equipo['region'] = region_icon.get_text(strip=True)

            # Propietario (Owner)
            owner_row = table.find('td', class_='infobox-label', string='Owner')
            if owner_row:
                owner_td = owner_row.find_next_sibling('td')
                equipo['propietario'] = owner_td.get_text(" ", strip=True)

            # Head Coach
            coach_row = table.find('td', class_='infobox-label', string='Head Coach')
            if coach_row:
                coach_td = coach_row.find_next_sibling('td')
                equipo['head_coach'] = coach_td.get_text(" ", strip=True)

            # Partners
            partners_row = table.find('td', class_='infobox-label', string='Partner')
            if partners_row:
                partners_td = partners_row.find_next_sibling('td')
                partners_links = partners_td.select('a')
                equipo['partners'] = equipo['partners'] = ", ".join([a.get_text(strip=True) for a in partners_links])

            # Fecha de fundaci贸n
            fecha_row = table.select_one('table.infobox-subtable td')
            if fecha_row:
                equipo['fecha_fundacion'] = fecha_row.get_text(strip=True)

            # Mostrar resultado
            print(f"Informaci贸n del equipo {equipo['nombre_equipo']} extra铆da correctamente.") 
            informacion_equipos.append(equipo)
        except Exception as e:
            print(f"Error al intentar extraer la informaci贸n del equipo: {e}")
            continue
    return informacion_equipos

if __name__ == "__main__":
    print(" Actualizando jugadores...")
    jugadores = get_player_data()
    print(jugadores)    

    
